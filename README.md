# WebDataParser
Python-скрипт для работы с веб-данными, их обработки и структурирования. Он предназначен для загрузки контента по списку URL, содержащихся в CSV-файле, и последующей обработки как веб-страниц, так и файлов для скачивания (PDF, DOCX и др.).

## Запуск скрипта

Для запуска скрипта необходимо установить все используемые библиотеки, которые перечислены в файле requirements.txt. Основные из них:
- requests — для выполнения HTTP-запросов и скачивания контента с веб-страниц и файлов.
- requests_html — расширение requests для работы с динамическим контентом, загружаемым через JavaScript.
- lxml_html_clean — для очистки HTML от нежелательных тегов и элементов, улучшая качество извлечённого текста.
- PyPDF2 — для извлечения текста и метаданных из PDF-документов.

После установки библиотек запустите скрипт, передав в качестве аргумента CSV-файл с URL:

`python3 Main.py tests1.csv`

## Начало работы
Скрипт принимает на вход путь к CSV-файлу с URL в качестве аргумента командной строки. Далее происходит очистка URL от лишних параметров, таких как трекинговые query-параметры, что позволяет работать с более «чистыми» и корректными ссылками. Этот этап реализован в классе URLProcessing.

## Загрузка контента
После этого в классе DownloadContent происходит загрузка контента, разделённого по типу: веб-страницы и файлы для скачивания.

Если URL указывает на файл (например, PDF или DOCX), скрипт скачивает его двумя способами: с помощью библиотеки requests и через системные утилиты wget. Все скачанные «сырые» файлы сохраняются в директорию raw_downloads/documents/.

Если URL ведёт на веб-страницу, скрипт загружает HTML-содержимое двумя методами: базовым (через requests) и более продвинутым, способным обрабатывать динамический контент, с использованием requests-html. Полученный HTML сохраняется в директорию raw_downloads/pages/.

В процессе выполнения HTTP-запросов к веб-страницам и файлам использовля реалистичный заголовок User-Agent, чтобы имитировать поведение обычного браузера и снизить риск блокировок со стороны серверов. Перед началом активного скачивания для каждого хоста проверяется наличие и содержимое файла robots.txt. Если в нём обнаружится ограничения на доступ к определённым URL, скрипт выводит предупреждение, но продолжал работу.

В отдельном текстовом файле anti_bot_notes.txt кратко описаны дополнительные методы и стратегии обхода защиты от ботов, которые известны и могут быть применены для повышения успешности сбора данных. Среди них — использование прокси-серверов, ротация User-Agent, управление сессиями и cookies.

## Обработка скаченных файлов
В классе ProcessingDownloadContent.py реализована обработка скачанных файлов и веб-страниц, а также сохранение результатов.

Для каждого успешно скачанного документа (не веб-страницы) извлекается текстовое содержимое. Для каждой успешно загруженной веб-страницы производится очистка основного текстового контента от HTML-тегов, скриптов, стилей и прочей разметки.

Очищенный текстовый контент сохраняется в отдельные .txt файлы в директорию processed_data/. Структура директорий raw_downloads/ и processed_data/, а также именование файлов организованы таким образом, чтобы обеспечить простое сопоставление "сырого" файла с его обработанной версией. Для этого используются уникальные идентификаторы.

## Формирование итогового реестра

Класс FormingResultsRegistry, отвечает за формирование итогового реестра — CSV-файла results_registry.csv. Этот реестр аккумулирует всю информацию о процессе обработки каждого URL и скачанных данных.

### Структура итогового реестра

| Поле                  | Описание                                                                                             |
|-----------------------|----------------------------------------------------------------------------------------------------|
| id                    | Уникальный порядковый номер или идентификатор записи                                               |
| source_url            | URL из входного CSV-файла                                                                           |
| final_url             | URL, с которого фактически был скачан контент (с учётом редиректов)                                |
| download_timestamp    | Дата и время скачивания в формате YYYY-MM-DD HH:MM:SS                                              |
| processing_timestamp  | Дата и время обработки в формате YYYY-MM-DD HH:MM:SS                                               |
| download_status       | Статус скачивания:                     |
| error_message         | Краткое описание ошибки, если она была                                                             |
| content_type_detected | Определённый тип контента: document или page                                                       |
| raw_file_path         | Относительный путь к сохранённому «сырому» файлу или странице                                      |
| processed_file_path   | Относительный путь к файлу с очищенным текстом                                                     |
| file_size_bytes       | Размер «сырого» файла в байтах, если применимо                                                     |
| document_page_count   | Количество страниц, если это документ и удалось определить                                         |
| detected_language     | Определённый язык документа или страницы                                                           |
| extracted_keywords    | Извлечённые ключевые слова через запятую, если применимо                                          |
| extracted_entities    | Опционально: извлечённые именованные сущности, если реализовывали                                  |
| summary               | Опционально: краткое содержание документа, если реализовывали                                      |
| metadata_author       | Автор из метаданных документа, если доступно                                                       |
| metadata_creation_date| Дата создания из метаданных документа, если доступно                                              |

В коде отсутствует обработка следующих столбцов: extracted_keywords, extracted_entities, summary, metadata_author, metadata_creation_date.