# WebDataParser
Python-скрипт для работы с веб-данными, их обработки и структурирования. Он предназначен для загрузки контента по списку URL, содержащихся в CSV-файле, и последующей обработки как веб-страниц, так и файлов для скачивания (PDF, DOCX и др.).

## Запуск скрипта

Для запуска скрипта необходимо установить все используемые библиотеки, которые перечислены в файле requirements.txt. Основные из них:
- requests — для выполнения HTTP-запросов и скачивания контента с веб-страниц и файлов.
- requests_html — расширение requests для работы с динамическим контентом, загружаемым через JavaScript.
- lxml_html_clean — для очистки HTML от нежелательных тегов и элементов, улучшая качество извлечённого текста.
- PyPDF2 — для извлечения текста и метаданных из PDF-документов.

После установки библиотек запустите скрипт, передав в качестве аргумента CSV-файл с URL:

`python3 Main.py tests1.csv`

## Начало работы
Скрипт принимает на вход путь к CSV-файлу с URL в качестве аргумента командной строки. Далее происходит очистка URL от лишних параметров, таких как трекинговые query-параметры, что позволяет работать с более «чистыми» и корректными ссылками. Этот этап реализован в классе URLProcessing.

## Загрузка контента
После этого в классе DownloadContent происходит загрузка контента, разделённого по типу: веб-страницы и файлы для скачивания.

Если URL указывает на файл (например, PDF или DOCX), скрипт скачивает его двумя способами: с помощью библиотеки requests и через системные утилиты wget. Все скачанные «сырые» файлы сохраняются в директорию raw_downloads/documents/.

Если URL ведёт на веб-страницу, скрипт загружает HTML-содержимое двумя методами: базовым (через requests) и более продвинутым, способным обрабатывать динамический контент, с использованием requests-html. Полученный HTML сохраняется в директорию raw_downloads/pages/.

В процессе выполнения HTTP-запросов к веб-страницам и файлам использовля реалистичный заголовок User-Agent, чтобы имитировать поведение обычного браузера и снизить риск блокировок со стороны серверов. Перед началом активного скачивания для каждого хоста проверяется наличие и содержимое файла robots.txt. Если в нём обнаружится ограничения на доступ к определённым URL, скрипт выводит предупреждение, но продолжал работу.

В отдельном текстовом файле anti_bot_notes.txt кратко описаны дополнительные методы и стратегии обхода защиты от ботов, которые известны и могут быть применены для повышения успешности сбора данных. Среди них — использование прокси-серверов, ротация User-Agent, управление сессиями и cookies.

## Обработка скаченных файлов
В классе ProcessingDownloadContent.py реализована обработка скачанных файлов и веб-страниц, а также сохранение результатов.

Для каждого успешно скачанного документа (не веб-страницы) извлекается текстовое содержимое. Для каждой успешно загруженной веб-страницы производится очистка основного текстового контента от HTML-тегов, скриптов, стилей и прочей разметки.

Очищенный текстовый контент сохраняется в отдельные .txt файлы в директорию processed_data/. Структура директорий raw_downloads/ и processed_data/, а также именование файлов организованы таким образом, чтобы обеспечить простое сопоставление "сырого" файла с его обработанной версией. Для этого используются уникальные идентификаторы.

## Текущая разработка
В данный момент в скрипт добовляется формирование итогового реестра

## Будущие доработки
В будущем можно будет доработать скачивания всех видов документов, а не только PDF. Также доработать базовую NLP-обработка извлеченного текста.